{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear & Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "Linear regression is defined as the approach to model a relationship between one or more input variables and one scalar output variable.\n",
    "For multiple inputs this statistic approach tries to estimate a function which \"solves\" the equations.\n",
    "\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "Logistic Regression is defined as the approach to model the distribution of dependent **discrete** variables. \n",
    "\n",
    "![](./linear-logistic-regression.png) - [Reference](https://www.saedsayad.com/logistic_regression.htm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement\n",
    "\n",
    "* Output from logistic regression can't be negative or greater than 1\n",
    "    * Neither could a probability\n",
    "* Logistic regression is great for categorical values (binary, classes, etc...)\n",
    "* Linear regression is great for continuous values (weight, number of hours, etc...) \n",
    "\n",
    "![](./logistic-regression-vs-linear.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "Given $x$, we want $\\hat{y} = P(y=1|x)$   \n",
    "\n",
    "with $0\\leq\\hat{y}\\leq1$ and $x \\in \\mathbb{R}^{n_x}$\n",
    "\n",
    "Parameters: $w \\in \\mathbb{R}^{n_x}, b \\in \\mathbb{R}$\n",
    "\n",
    "Output: $\\hat{y}= \\sigma(z)$\n",
    "\n",
    "with $z = w^Tx+b$\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "$\\sigma{(z)} = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "If x is large: $\\sigma(z) \\approx \\frac{1}{1+very small} = 1$\n",
    "\n",
    "If x is a large negative: $\\sigma(z) \\approx \\frac{1}{1+very large} = 0$\n",
    "\n",
    "![](./sigmoid.png)\n",
    "\n",
    "Other activation functions\n",
    "\n",
    "![](./activation-functions.png)\n",
    "\n",
    "### Loss-function\n",
    "\n",
    "\"Linear regression uses mean squared error as its cost function. If this is used for logistic regression, then it will be a non-convex function of parameters (theta). Gradient descent will converge into global minimum only if the function is convex.\" -[Reference](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc) \n",
    "\n",
    "![](./convex-non-convex.png)\n",
    "\n",
    "For logistic regression the cost function is defined as follows:\n",
    "\n",
    "$L(\\hat{y},y) = -(y\\log{\\hat{y}}+(1-y)\\log{(1-\\hat{y})})$\n",
    "\n",
    "If $y=1$: $L(\\hat{y},y) = -\\log{\\hat{y}} \\leftarrow$ want $\\log{\\hat{y}}$ large, want $\\hat{y}$ large.\n",
    "\n",
    "If $y=0$: $L(\\hat{y},y) = -\\log{(1-\\hat{y})} \\leftarrow$ want $\\log{\\hat{y}}$ large, want $\\hat{y}$ small.\n",
    "\n",
    "### Cost-function\n",
    "\n",
    "$J(w,b)=\\frac{1}{m}\\sum^m_{i=1}{L(\\hat{y}^{(i)},y^{(i)})} = \\frac{1}{m}\\sum^m_{i=1}[y^{(i)}\\log{\\hat{y}^{(i)}}+(1-y^{(i)})\\log{(1-\\hat{y}^{(i)})}]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W, B):\n",
    "    return sigmoid(np.dot(W.T, X) + B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A, Y):\n",
    "    m = Y.shape\n",
    "    return -1/m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W, B):\n",
    "    # retrieve example size\n",
    "    m = X.shape(1)\n",
    "    \n",
    "    # forward\n",
    "    A = forward(X, W, B)\n",
    "    \n",
    "    # for binary classification\n",
    "    # create output array of size (1,m)\n",
    "    Y_pred = np.zeros((1, m))\n",
    "    \n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        Y_pred[A > 0.5] = 1\n",
    "        Y_pred[A <= 0.5] = 0\n",
    "        \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "1. [Linear Regression - Wiki](https://en.wikipedia.org/wiki/Linear_regression)\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "1. [Logistic Regression - saedsayad](https://www.saedsayad.com/logistic_regression.htm)\n",
    "2. [(Univariate|Simple) Logistic regression](https://gerardnico.com/data_mining/simple_logistic_regression)\n",
    "3. [Sigmoid - Wiki](https://en.wikipedia.org/wiki/Sigmoid_function)\n",
    "4. [Logistic Regression — Detailed Overview](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
